{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/avacaondata/nlpboost/blob/main/notebooks/extractive_qa/train_sqac.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Question Answering in Spanish: SQAC\n",
    "\n",
    "In this tutorial we will see how we can train multiple Spanish models on a QA dataset in that language: SQAC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the needed modules or, if you are running this notebook in Google colab, please uncomment the cell below and run it before importing, in order to install `nlpboost`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alejandro.vaca/miniconda3/envs/largesum/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alejandro.vaca/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/alejandro.vaca/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nlpboost import AutoTrainer, ModelConfig, DatasetConfig, ResultsPlotter\n",
    "from transformers import EarlyStoppingCallback\n",
    "from nlpboost.default_param_spaces import hp_space_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the dataset\n",
    "\n",
    "The next step is to define the fixed train args, which will be the `transformers.TrainingArguments` passed to `transformers.Trainer` inside `nlpboost.AutoTrainer`. For a full list of arguments check [TrainingArguments documentation](https://huggingface.co/docs/transformers/v4.25.1/en/main_classes/trainer#transformers.TrainingArguments). `DatasetConfig` expects these arguments in dictionary format.\n",
    "\n",
    "To save time, we set `max_steps` to 1; in a real setting we would need to define these arguments differently. However, that is out of scope for this tutorial. To learn how to work with Transformers, and how to configure the training arguments, please check Huggingface Course on NLP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_train_args = {\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"num_train_epochs\": 10,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"logging_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"save_total_limit\": 2,\n",
    "        \"seed\": 69,\n",
    "        \"fp16\": True,\n",
    "        \"dataloader_num_workers\": 8,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"per_device_eval_batch_size\": 16,\n",
    "        \"adam_epsilon\": 1e-6,\n",
    "        \"adam_beta1\": 0.9,\n",
    "        \"adam_beta2\": 0.999,\n",
    "        \"max_steps\": 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define some common args for the dataset. In this case we minimize the loss, as for QA no compute metrics function is used during training. We use the loss to choose the best model and then compute metrics over the test set, which is not a straightforward process (that is the reason for not computing metrics in-training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args_dataset = {\n",
    "        \"seed\": 44,\n",
    "        \"direction_optimize\": \"minimize\",\n",
    "        \"metric_optimize\": \"eval_loss\",\n",
    "        \"retrain_at_end\": False,\n",
    "        \"callbacks\": [EarlyStoppingCallback(1, 0.00001)],\n",
    "        \"fixed_training_args\": fixed_train_args\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define arguments specific of SQAC. In this case, the text field and the label col are not used, so we just set them to two string columns of the dataset. In QA tasks, `nlpboost` assumes the dataset is in SQUAD format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqac_config = default_args_dataset.copy()\n",
    "sqac_config.update(\n",
    "    {\n",
    "        \"dataset_name\": \"sqac\",\n",
    "        \"alias\": \"sqac\",\n",
    "        \"task\": \"qa\",\n",
    "        \"text_field\": \"context\",\n",
    "        \"hf_load_kwargs\": {\"path\": \"PlanTL-GOB-ES/SQAC\"},\n",
    "        \"label_col\": \"question\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqac_config = DatasetConfig(**sqac_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Models\n",
    "\n",
    "We will configure three Spanish models. As you see, we only need to define the `name`, which is the path to the model (either in HF Hub or locally), `save_name` which is an arbitrary name for the model, the hyperparameter space and the number of trials. There are more parameters, which you can check in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertin_config = ModelConfig(\n",
    "        name=\"bertin-project/bertin-roberta-base-spanish\",\n",
    "        save_name=\"bertin\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1,\n",
    ")\n",
    "beto_config = ModelConfig(\n",
    "        name=\"dccuchile/bert-base-spanish-wwm-cased\",\n",
    "        save_name=\"beto\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1,\n",
    ")\n",
    "albert_config = ModelConfig(\n",
    "        name=\"CenIA/albert-tiny-spanish\",\n",
    "        save_name=\"albert\",\n",
    "        hp_space=hp_space_base,\n",
    "        n_trials=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('largesum': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bac692fd94dcfa608ba1aabbfbe7d5467f50ca857b57fe228a116df0c8b5b792"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
